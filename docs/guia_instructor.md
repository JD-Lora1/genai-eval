# GuÃ­a del Instructor: EvaluaciÃ³n de IA Generativa

## SecciÃ³n 1: Â¡Hola, Facilitador!

Â¡Bienvenido a esta aventura de aprendizaje! Esta guÃ­a estÃ¡ diseÃ±ada para ayudarte a facilitar una sesiÃ³n sÃºper interesante sobre cÃ³mo evaluar sistemas de inteligencia artificial generativa (*Generative AI*).

Â¿Por quÃ© usamos este enfoque? Porque creemos que aprender debe ser como armar un rompecabezas: primero entiendes para quÃ© sirve cada pieza (la teorÃ­a), luego practicas armÃ¡ndolo (el taller), y al final puedes ver la imagen completa (la comprensiÃ³n profunda).

Tu papel es ser el guÃ­a que ayuda a los estudiantes a conectar los puntos entre lo que aprenden en la presentaciÃ³n y lo que experimentan en el cÃ³digo. Â¡No te preocupes si no eres un experto absoluto! Esta guÃ­a te darÃ¡ todo lo que necesitas.

---

## SecciÃ³n 2: Â¿QuÃ© es todo esto?

### IA Generativa (*Generative AI*)
ImagÃ­nate que tienes un robot muy inteligente que puede escribir historias, traducir idiomas, o resumir textos largos. Ese robot usa IA generativa. Pero aquÃ­ viene el problema: **Â¿cÃ³mo sabemos si el robot estÃ¡ haciendo un buen trabajo?**

Es como cuando un niÃ±o te cuenta una historia que inventÃ³. TÃº puedes decir si es buena o no, pero Â¿cÃ³mo le explicas eso a una computadora? Â¡AhÃ­ es donde entran las mÃ©tricas de evaluaciÃ³n!

### MÃ©tricas AutomÃ¡ticas (*Automatic Metrics*)
Son como reglas matemÃ¡ticas que la computadora usa para "calificar" el trabajo de la IA. Las dos mÃ¡s famosas son:

**MÃ©tricas de N-gramas:**

**BLEU (*Bilingual Evaluation Understudy*)**: 
- Es como un profesor que cuenta cuÃ¡ntas palabras estÃ¡n en el lugar correcto
- Se fija mucho en la precisiÃ³n: "Â¿Las palabras que usaste son las correctas?"
- Problema: No se preocupa si faltaron cosas importantes

**ROUGE (*Recall-Oriented Understudy for Gisting Evaluation*)**:
- Es como un profesor que se asegura de que no se te olvidÃ³ nada importante
- Se fija mÃ¡s en la cobertura: "Â¿Incluiste todo lo que debÃ­as incluir?"
- Es sÃºper Ãºtil para evaluar resÃºmenes

**MÃ©tricas basadas en embeddings:**

**BERTScore**: 
- Es como tener un profesor que entiende sinÃ³nimos
- Sabe que "pelÃ­cula" y "film" significan lo mismo
- Usa inteligencia artificial (BERT) para entender el significado

**MÃ©tricas aprendidas:**

**BLEURT**: 
- Es como un profesor entrenado especÃ­ficamente para calificar como los humanos
- Ha "estudiado" millones de ejemplos de evaluaciones humanas
- Es la mÃ¡s sofisticada pero tambiÃ©n la mÃ¡s lenta

### EvaluaciÃ³n Humana (*Human Evaluation*)
A veces las computadoras no pueden juzgar cosas como:
- Si un texto suena natural (*fluency*)
- Si tiene sentido lÃ³gico (*coherence*)
- Si es creativo o aburrido
- Si los hechos son correctos

Para esto necesitamos humanos reales que lean y califiquen los textos.

**Marco MQM (*Multidimensional Quality Metrics*)**:
- Es como tener una rÃºbrica estÃ¡ndar para evaluar
- MQM-lite usa tres dimensiones principales:
  - *Adequacy*: Â¿Se preservÃ³ bien el contenido original?
  - *Fluency*: Â¿Suena natural y es gramaticalmente correcto?
  - *Consistency*: Â¿Es coherente internamente?

---

## SecciÃ³n 3: El Plan de Juego (Minuto a Minuto)

### **PreparaciÃ³n (5 minutos antes de empezar)**
- [ ] AsegÃºrate de que todos puedan acceder al notebook (Google Colab)
- [ ] Ten la presentaciÃ³n lista
- [ ] Prepara ejemplos adicionales por si alguien pregunta

### **IntroducciÃ³n y MotivaciÃ³n (10 minutos)**
**QuÃ© hacer:**
- Empieza con la pregunta del millÃ³n: "Â¿CÃ³mo sabemos si nuestra IA es buena?"
- Usa la analogÃ­a de comparar estudiantes: imaginen que tienen que decidir cuÃ¡l de dos estudiantes escribiÃ³ el mejor ensayo

**SeÃ±al de que va bien:** Los estudiantes empiezan a proponer ideas como "leemos los textos" o "comparamos con un ejemplo perfecto"

**Si se pone lento:** Pregunta "Â¿CÃ³mo calificarÃ­an ustedes un ensayo? Â¿Y si tuvieran que calificar 1000 ensayos en una hora?"

### **MÃ³dulo 1: MÃ©tricas N-gram - BLEU y ROUGE (15 minutos)**
**PresentaciÃ³n (8 minutos):**
- Explica las mÃ©tricas n-gram como "los contadores de palabras"
- BLEU = precisiÃ³n, ROUGE = cobertura
- **Momento clave:** Muestra cÃ³mo fallan con sinÃ³nimos: "pelÃ­cula" vs "film"
- Enfatiza: "Son rÃ¡pidas y simples, pero tienen limitaciones"

**Taller (7 minutos):**
- Los estudiantes experimentan con BLEU y ROUGE
- Diles: "Prueben con sinÃ³nimos y cambios de orden - Â¿quÃ© pasa?"
- **Pregunta clave:** "Â¿Por quÃ© las mÃ©tricas n-gram luchan con parÃ¡frasis?"

**SeÃ±ales de Ã©xito:** Comentarios como "Â¡No entiende que 'auto' y 'carro' son lo mismo!"

### **MÃ³dulo 2: BERTScore - MÃ©tricas con Embeddings (15 minutos)**
**PresentaciÃ³n (8 minutos):**
- Empieza con el problema de n-grams: "Â¿QuÃ© pasa cuando digo 'film' en lugar de 'movie'?"
- Presenta BERTScore como "el evaluador que entiende significados"
- **AnalogÃ­a clave:** Es como tener un traductor inteligente en tu cerebro
- Explica: usa BERT para entender que palabras diferentes pueden significar lo mismo

**Taller (7 minutos):**
- Comparar BERTScore vs mÃ©tricas n-gram
- **Ejercicio:** Casos donde n-grams fallan pero BERTScore funciona
- **Pregunta:** "Â¿CuÃ¡ndo vale la pena usar mÃ©tricas mÃ¡s lentas pero mÃ¡s inteligentes?"

### **MÃ³dulo 3: BLEURT - MÃ©tricas Aprendidas (10 minutos)**
**PresentaciÃ³n (5 minutos):**
- **EvoluciÃ³n:** "Hemos ido de contar palabras â†’ entender significados â†’ aprender de humanos"
- Presenta BLEURT como "la mÃ©trica entrenada para pensar como humano"
- **AnalogÃ­a:** Es como un estudiante de doctorado que ha leÃ­do millones de evaluaciones
- Explica trade-offs: mÃ¡s precisa pero mÃ¡s lenta y compleja

**Taller (5 minutos):**
- ComparaciÃ³n rÃ¡pida de todas las mÃ©tricas
- **Ejercicio:** Ver todas las mÃ©tricas en un mismo ejemplo
- **Pregunta:** "Â¿CuÃ¡ndo vale la pena la complejidad extra?"

### **MÃ³dulo 4: EvaluaciÃ³n Humana con MQM (15 minutos)**
**PresentaciÃ³n (5 minutos):**
- Introduce el marco MQM: "Una rÃºbrica estÃ¡ndar para evaluar IA"
- Explica MQM-lite: AdecuaciÃ³n, Fluidez, Consistencia
- **ConexiÃ³n:** "Las mÃ¡quinas miden palabras, los humanos miden experiencia"

**Actividad Grupal (10 minutos):**
- Forma grupos de 3-4 personas
- **Instrucciones:** "Usen el marco MQM para evaluar dos textos"
- **Proceso:** Lean â†’ Discutan â†’ Califiquen en las 3 dimensiones
- **Circula y escucha:** Material para la discusiÃ³n final

**Truco del instructor:** Celebra los desacuerdos: "Â¡Perfecto! AsÃ­ funciona la evaluaciÃ³n humana real"

### **MÃ³dulo 5: La Gran ComparaciÃ³n (10 minutos)**
**El momento "Aha!" (8 minutos):**
- Los estudiantes comparan sus evaluaciones MQM con TODAS las mÃ©tricas automÃ¡ticas
- **VisualizaciÃ³n:** GrÃ¡ficos lado a lado de humanos vs. mÃ¡quinas
- **Pregunta clave:** "Â¿QuÃ© mÃ©trica se parece mÃ¡s a su juicio humano? Â¿Por quÃ©?"

**Cierre (2 minutos):**
- Resume: "No hay mÃ©trica perfecta - cada una tiene su lugar"
- **Pregunta final:** "Â¿CÃ³mo combinarÃ­an estas mÃ©tricas en un proyecto real?"

### **Preguntas y DiscusiÃ³n (5 minutos)**

---

## SecciÃ³n 4: Consejos Clave

### **Para mantener la energÃ­a alta:**
- **Usa analogÃ­as simples:** BLEU = contador de palabras, ROUGE = detector de cosas faltantes, Humanos = jueces con corazÃ³n
- **Celebra los errores:** Cuando algo sale "mal" en el cÃ³digo, di "Â¡Perfecto! Eso nos enseÃ±a algo importante"
- **Haz preguntas constantemente:** En lugar de explicar todo, pregunta "Â¿QuÃ© creen que va a pasar si...?"

### **Problemas tÃ©cnicos comunes:**
- **El notebook no carga:** Ten un Plan B con los resultados pre-calculados
- **Error de instalaciÃ³n:** Google Colab usualmente funciona, pero ten screenshots de los resultados esperados
- **Estudiantes perdidos:** AsÃ­gna "compaÃ±eros de laboratorio" - que trabajen en pares

### **Dudas frecuentes y cÃ³mo responderlas:**
**"Â¿Por quÃ© las mÃ©tricas automÃ¡ticas dan resultados raros?"**
- Respuesta: "Cada mÃ©trica 've' algo diferente: BLEU cuenta palabras exactas, BERTScore entiende significados, BLEURT imita humanos. Es como tener diferentes tipos de doctores para diferentes sÃ­ntomas"

**"Â¿CuÃ¡l mÃ©trica es la mejor?"**
- Respuesta: "Depende de tu 'receta': Â¿Necesitas velocidad? â†’ n-grams. Â¿PrecisiÃ³n semÃ¡ntica? â†’ BERTScore. Â¿CorrelaciÃ³n humana? â†’ BLEURT. Â¿EvaluaciÃ³n completa? â†’ Combinar todas"

**"Â¿Por quÃ© no usar solo BLEURT si es la mÃ¡s avanzada?"**
- Respuesta: "Es como preguntar por quÃ© no usar siempre un Ferrari. A veces necesitas algo mÃ¡s simple, barato y rÃ¡pido para hacer el mandado"

**"Â¿CÃ³mo saber si mis mÃ©tricas automÃ¡ticas son confiables?"**
- Respuesta: "CompÃ¡ralas con evaluaciÃ³n humana en una muestra pequeÃ±a. Si coinciden, puedes confiar. Si no, ajusta tu estrategia"

### **SeÃ±ales de que todo va bien:**
- Los estudiantes hacen preguntas espontÃ¡neas
- Escuchas "Â¡Ah, ya entendÃ­!" durante los ejercicios
- Hay debate durante la evaluaciÃ³n humana
- Al final, pueden explicar cuÃ¡ndo usarÃ­an cada mÃ©trica

### **SeÃ±ales de alarma:**
- Silencio total durante las actividades
- Nadie hace preguntas
- Los estudiantes no participan en la evaluaciÃ³n grupal

**Â¿QuÃ© hacer si algo no funciona?**
- **ReenfÃ³cate en lo conceptual:** Si la tecnologÃ­a falla, usa ejemplos en la pizarra
- **Usa la analogÃ­a de calificar exÃ¡menes:** Todos entienden la diferencia entre contar errores ortogrÃ¡ficos vs. evaluar la creatividad
- **Haz mÃ¡s interactivo:** Convierte la sesiÃ³n en un debate sobre "Â¿CÃ³mo calificarÃ­an ustedes a ChatGPT?"

### **Mensaje final para ti, facilitador:**
Recuerda que no tienes que ser un experto perfecto. Tu trabajo es guiar la curiosidad de los estudiantes y ayudarles a descubrir estos conceptos por sÃ­ mismos. Â¡Los mejores maestros son los que aprenden junto con sus estudiantes!

**Â¡TÃº puedes hacer esto! ğŸš€**
