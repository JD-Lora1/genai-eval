<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0" />
  <title>Evaluation Tools and Metrics for Generative AI</title>

  <!-- Reveal.js core and plugins -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.6.1/reveal.min.css"/>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.6.1/plugin/highlight/monokai.min.css"/>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css"/>

  <style>
    :root {
      /* Palette: minimalist pastels + neutrals */
      --bg-white: #ffffff;
      --ink: #0b1220;           /* near-black text for contrast */
      --muted: #4b5563;         /* gray for secondary text */
      --glass-fill-1: rgba(255,255,255,0.72);
      --glass-fill-2: rgba(255,255,255,0.52);
      --glass-stroke-1: rgba(255,255,255,0.85);
      --glass-stroke-2: rgba(255,255,255,0.28);
      --shadow-ambient: 0 24px 60px rgba(15,23,42,0.08);

      /* Soft warm/cool light blobs (very low saturation for elegance) */
      --light-amber: rgba(255, 196, 131, 0.16);
      --light-rose:  rgba(255, 163, 176, 0.14);
      --light-sky:   rgba(171, 207, 255, 0.16);
      --light-lilac: rgba(198, 183, 255, 0.14);

      /* Layout */
      --top-blank: clamp(64px, 8vh, 128px);   /* blank area at top of each slide */
      --side-pad: clamp(16px, 4vw, 56px);
      --radius-xxl: 28px;
    }

    /* Page background with soft light blobs */
    body {
      margin: 0;
      background:
        radial-gradient(700px 700px at 80% -10%, var(--light-sky), transparent 65%),
        radial-gradient(560px 560px at -10% 15%, var(--light-rose), transparent 60%),
        radial-gradient(640px 640px at 50% 110%, var(--light-amber), transparent 65%),
        radial-gradient(720px 720px at 10% 85%, var(--light-lilac), transparent 65%),
        var(--bg-white);
      font-family: 'Roboto', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
      color: var(--ink);
    }

    /* Reveal base tweaks */
    .reveal { color: var(--ink); }
    .reveal .slides { width: 100vw; height: 100vh; }

    /* Each slide gets a generous top blank area and a frosted-glass panel behind content */
    .reveal .slides section {
      padding: var(--top-blank) var(--side-pad) var(--side-pad);
      box-sizing: border-box;
      display: flex;
      flex-direction: column;
      justify-content: flex-start; /* start lower so top stays blank */
      position: relative;
      overflow: hidden;
    }

    /* Frosted panel: auto-placed behind slide content (except module-title which already has its own bg) */
    .reveal .slides section:not(.module-title):not(.cover):not(.no-glass)::before {
      content: '';
      position: absolute;
      top: calc(var(--top-blank) - 20px);
      left: 50%;
      transform: translateX(-50%);
      width: min(1100px, 90%);
      min-height: min(72vh, 720px);
      border-radius: var(--radius-xxl);
      background: linear-gradient(180deg, var(--glass-fill-1), var(--glass-fill-2));
      backdrop-filter: blur(20px) saturate(120%);
      -webkit-backdrop-filter: blur(20px) saturate(120%);
      box-shadow: var(--shadow-ambient);
      z-index: -1;
    }

    /* Subtle soft edge highlight around frosted panel */
    .reveal .slides section:not(.module-title):not(.cover):not(.no-glass)::after {
      content: '';
      position: absolute; inset: 0;
      top: calc(var(--top-blank) - 20px);
      left: 50%; transform: translateX(-50%);
      width: min(1100px, 90%);
      min-height: min(72vh, 720px);
      border-radius: var(--radius-xxl);
      padding: 1px;
      background: linear-gradient(140deg, var(--glass-stroke-1), var(--glass-stroke-2));
      -webkit-mask: linear-gradient(#000 0 0) content-box, linear-gradient(#000 0 0);
      -webkit-mask-composite: xor; mask-composite: exclude;
      pointer-events: none;
      z-index: -1;
    }

    /* Typography */
    .reveal h1 { font-size: clamp(2.6rem, 6vw, 4.4rem); margin: 0.2em 0 0.3em; font-weight: 700; }
    .reveal h2 { font-size: clamp(1.8rem, 4vw, 3rem);  margin: 0.2em 0 0.6em; font-weight: 600; color: #0e3a6f; }
    .reveal h3 { font-size: clamp(1.4rem, 3vw, 2rem);  margin: 0.6em 0 0.4em; font-weight: 500; color: #1f4b7a; }
    .reveal p, .reveal li { font-size: clamp(1.05rem, 2.2vw, 1.35rem); line-height: 1.6; color: var(--muted); }
    .reveal strong { color: var(--ink); font-weight: 700; }

    /* Title/Cover slide with soft spotlight */
    .cover {
      align-items: center; text-align: center;
    }
    .cover::before, .cover::after { display: none; }
    .cover .hero {
      position: relative;
      width: min(1100px, 86%);
      padding: 3.5rem 2.8rem;
      border-radius: 32px;
      background: linear-gradient(180deg, var(--glass-fill-1), var(--glass-fill-2));
      backdrop-filter: blur(22px) saturate(120%);
      -webkit-backdrop-filter: blur(22px) saturate(120%);
      box-shadow: var(--shadow-ambient);
      margin: 1.2rem auto 0;
    }
    .cover .beam {
      position: absolute; inset: -10% -20% auto -20%; height: 60vh; pointer-events: none;
      background: radial-gradient(closest-side, rgba(0,118,255,0.14), transparent 70%);
      filter: blur(42px); z-index: -1;
    }

    /* Utility glass card for callouts/tables inside slides */
    .glass-card {
      background: linear-gradient(180deg, var(--glass-fill-1), var(--glass-fill-2));
      backdrop-filter: blur(16px) saturate(120%);
      -webkit-backdrop-filter: blur(16px) saturate(120%);
      border-radius: 22px; padding: 1.2rem 1.2rem; margin: 0.8rem 0;
      box-shadow: var(--shadow-ambient);
    }

    .reveal table { width: 100%; border-collapse: separate; border-spacing: 0; overflow: hidden; border-radius: 16px; background: rgba(255,255,255,0.66); backdrop-filter: blur(12px); }
    .reveal th { text-align: left; padding: 0.8rem 1rem; background: rgba(15, 23, 42, 0.06); color: var(--ink); }
    .reveal td { padding: 0.75rem 1rem; border-top: 1px solid rgba(15, 23, 42, 0.06); }

    .reveal .progress { height: 4px; color: #0e3a6f; }
    .reveal .controls { color: #0e3a6f; }

    /* Keep content inside panel readable on small devices */
    @media (max-width: 820px) {
      .reveal .slides section::before,
      .reveal .slides section::after { width: 92%; min-height: 70vh; }
    }
  </style>
</head>
<body>
  <div class="reveal">
    <div class="slides">

      <!-- Cover slide built in HTML so we can craft the hero panel -->
      <section class="cover">
        <div class="beam"></div>
        <div class="hero">
          <h1>Evaluation Tools and Metrics for Generative AI</h1>
          <h2>How Do We Know If Our AI Is Actually Good?</h2>
          <p style="margin-top: 1rem; font-size: 1.15rem;">Elegant, modern, and light â€” a frosted glass presentation with soft, warm lighting.</p>
        </div>
      </section>

      <!-- Markdown-driven slides (content inlined to avoid local XHR issues) -->
      <section data-markdown data-separator="^---$" data-separator-vertical="^##\s" data-charset="utf-8">
        <textarea data-template>
# Evaluation Tools and Metrics for Generative AI
## How Do We Know If Our AI Is Actually Good?

---

## The Problem Statement

Imagine you've just built the next ChatGPT. ğŸ¤–

Your AI can write essays, answer questions, and even compose poetry.

But here's the million-dollar question: **How do you prove it's better than the competition?**

---

## Why This Matters

* Companies spend millions on AI development
* Users need reliable, high-quality AI systems
* Researchers need to measure progress
* **You need objective ways to compare AI performance**

Without proper evaluation, we're just guessing! ğŸ²

---

## Our Journey Today

1. **N-gram Metrics** - BLEU & ROUGE
2. **Embedding-based Metrics** - BERTScore
3. **Learned Metrics** - BLEURT
4. **Human Evaluation** - MQM Framework
5. **Combining Approaches** - The complete picture

---

# Module 1: N-gram Based Metrics
## The Foundation of Automatic Evaluation

---

## Why Automatic Metrics?

**The Challenge:** Human evaluation is:
- â° Time-consuming
- ğŸ’° Expensive
- ğŸ”„ Hard to reproduce
- ğŸ“ Subjective

**The Solution:** Automatic metrics that can:
- âš¡ Run instantly
- ğŸ’¸ Cost almost nothing
- ğŸ” Give consistent results
- ğŸ“Š Provide objective scores

---

## Enter BLEU Score

**BLEU** = **B**i**l**ingual **E**valuation **U**nderstudy

Created by IBM in 2002 for machine translation evaluation.

**Key components:**
- n-gram precision (1-4 grams)
- Brevity penalty (to prevent gaming with short outputs)

[ğŸ“– Original BLEU Paper](https://aclanthology.org/P02-1040.pdf)

---

## BLEU: The Core Idea

BLEU measures **precision** of n-grams (word sequences):

**Reference:** "The cat is on the mat"
**Candidate:** "The cat is on the"

**1-gram precision:** 5/5 = 100% âœ…
**2-gram precision:** 4/4 = 100% âœ…

*But wait... something's missing!* ğŸ¤”

---

## BLEU in Action: Simple Example

**Reference:** "I love machine learning"
**Candidate 1:** "I love machine learning" â†’ BLEU â‰ˆ 1.0 ğŸ¯
**Candidate 2:** "Machine learning I love" â†’ BLEU â‰ˆ 0.6 ğŸ“‰
**Candidate 3:** "I hate machine learning" â†’ BLEU â‰ˆ 0.7 ğŸ¤·

*Why these scores? Let's find out!*

---

## ğŸš€ Call to Action: Hands-On Time!

**Open your workshop notebook now!**

ğŸ‘¨â€ğŸ’» **Exercise 1:** Calculate BLEU scores for different text pairs
- Experiment with word order
- See how BLEU reacts to synonyms
- Discover BLEU's limitations

**Time:** 7 minutes

---

# Module 2: ROUGE Metrics
## A Better Tool for Different Tasks

---

## BLEU's Limitation

**BLEU Problem:** Only measures precision, ignores recall

**Example:**
- **Reference:** "The quick brown fox jumps over the lazy dog"
- **Candidate:** "The fox" 
- **BLEU:** Relatively high (both words match!)
- **Reality:** Missing 80% of the content! ğŸ˜±

---

## Introducing ROUGE

**ROUGE** = **R**ecall-**O**riented **U**nderstudy for **G**isting **E**valuation

Developed for summarization tasks by Chin-Yew Lin (2004).

[ğŸ“– ROUGE Paper](https://aclanthology.org/W04-1013.pdf)

---

## ROUGE Variants

**ROUGE-1:** Unigram overlap (individual words)
**ROUGE-2:** Bigram overlap (word pairs)
**ROUGE-L:** Longest common subsequence (LCS)
**ROUGE-W:** Weighted longest common subsequence

**Focus on ROUGE-L:**
- Identifies longest sequence of matching words
- Order matters, but words don't need to be consecutive
- More flexible than fixed n-grams

*Each serves different evaluation needs!*

---

## N-gram Metrics: Strengths & Limitations

**Strengths:**
- âœ… Simple, fast, and reproducible
- âœ… No training required
- âœ… Language-agnostic (mostly)
- âœ… Correlate reasonably with human judgment on some tasks

**Limitations:**
- âŒ Insensitive to synonyms and paraphrases
- âŒ Penalize word order changes even when meaning preserved
- âŒ Miss semantic similarity
- âŒ Poor for creative generation tasks

## ROUGE vs BLEU: Key Differences

| Aspect | BLEU | ROUGE |
|--------|------|-------|
| Focus | Precision | Recall |
| Best for | Translation | Summarization |
| Measures | N-gram overlap | Various overlaps |
| Penalty | Brevity | None |

---

## ROUGE in Practice

**Original Text:** "Artificial intelligence is transforming how we work, learn, and communicate in the modern world."

**Summary 1:** "AI transforms work and communication."
**Summary 2:** "Technology changes everything."

*Which summary is better? Let ROUGE decide!*

---

## ğŸš€ Call to Action: ROUGE Workshop

**Back to your notebook!**

ğŸ‘¨â€ğŸ’» **Exercise 2:** Compare summaries using ROUGE
- Calculate ROUGE-1 and ROUGE-L scores
- Analyze different summary qualities
- Compare with your intuition

**Time:** 8 minutes

---

# Module 2: Embedding-Based Metrics
## Going Beyond N-grams

---

## Why N-grams Aren't Enough

**Example:**
- **Reference:** "The movie was excellent."
- **Output 1:** "The film was excellent." (Should be high score)
- **Output 2:** "The movie was terrible." (Should be low score)

**BLEU/ROUGE:** Output 2 scores higher than Output 1! ğŸ¤¦

**We need metrics that understand meaning!**

---

## Enter BERTScore

**BERTScore** uses contextual embeddings from pre-trained language models (BERT).

**How it works:**
1. Convert words to vectors using BERT
2. Calculate cosine similarity between vectors
3. Find best matches between reference and candidate words
4. Compute precision, recall, and F1 scores

[ğŸ“– BERTScore Paper](https://arxiv.org/abs/1904.09675)

---

## BERTScore in Action

**Reference:** "The movie was excellent."
**Output 1:** "The film was excellent." (High BERTScore â‰ˆ 0.95)
**Output 2:** "The movie was terrible." (Low BERTScore â‰ˆ 0.75)

**Why it works:** "film" and "movie" have similar embeddings!

---

## BERTScore: Strengths & Limitations

**Strengths:**
- âœ… Captures semantic meaning and paraphrases
- âœ… Contextual understanding of words
- âœ… Higher correlation with human judgments

**Limitations:**
- âŒ Slower than n-gram metrics
- âŒ Depends on pre-trained model quality and biases
- âŒ Requires more computational resources

---

## ğŸš€ Call to Action: BERTScore Workshop

**Back to your notebook!**

ğŸ‘¨â€ğŸ’» **Exercise 3:** Compare BERTScore with BLEU/ROUGE
- See how it handles paraphrases and synonyms
- Compare scores on examples where n-gram metrics fail
- Understand when to use embedding-based metrics

**Time:** 7 minutes

---

# Module 3: Learned Metrics
## Training Models to Evaluate Like Humans

---

## The Evolution of Metrics

**Traditional Metrics** â†’ **Embedding Metrics** â†’ **Learned Metrics**

**Why learn from humans?**
- Human evaluation is still the gold standard
- Different aspects matter for different tasks
- Automatic metrics should predict human preferences

---

## Introducing BLEURT

**BLEURT** = **B**ilingual **E**valuation **U**nderstudy with **R**epresentations from **T**ransformers

Developed by Google Research (2020).

**How it works:**
1. Start with a pre-trained BERT model
2. Fine-tune on millions of synthetic examples
3. Further fine-tune on human annotations

[ğŸ“– BLEURT Paper](https://arxiv.org/abs/2004.04696)

---

## BLEURT: Strengths & Limitations

**Strengths:**
- âœ… Highest correlation with human judgments
- âœ… Robust to noise and edge cases
- âœ… Can be customized for specific tasks

**Limitations:**
- âŒ Requires fine-tuning
- âŒ Less interpretable (black box)
- âŒ Computationally intensive
- âŒ May inherit biases from training data

---

## ğŸš€ Call to Action: Learned Metrics Workshop

**Back to your notebook!**

ğŸ‘¨â€ğŸ’» **Exercise 4:** Explore BLEURT scores
- Compare with other metrics
- Analyze correlation with intuition
- Discuss when to use learned metrics

**Time:** 5 minutes

---

# Module 4: Human Evaluation
## When Machines Aren't Enough

---

## Why Humans Still Matter

**Automatic metrics miss:**
- ğŸ­ Fluency and naturalness
- ğŸ§  Coherence and logic
- ğŸ¯ Relevance to context
- ğŸ’­ Creativity and style
- ğŸ“š Factual accuracy

*Some things only humans can judge properly!*

---

## The MQM Framework

**MQM** = **M**ultidimensional **Q**uality **M**etrics

A standardized framework for human evaluation.

**MQM-lite:** Simplified version with three key dimensions:

1. **Adequacy:** How well is the original content preserved?
2. **Fluency:** How natural and grammatically correct is the text?
3. **Consistency:** Is the text internally coherent and consistent?

---

## Types of Human Evaluation

**1. Likert Scales**
- Rate from 1-5 or 1-7
- "How fluent is this text?"

**2. Pairwise Comparison**
- "Which text is better, A or B?"
- More reliable than absolute ratings

**3. Ranking Tasks**
- Order multiple options from best to worst

---

## Human Evaluation Challenges

**ğŸ˜´ Annotator Fatigue**
- Quality decreases over time

**ğŸ­ Subjectivity**
- Different people, different opinions

**ğŸ’° Cost & Time**
- Expensive and slow

**ğŸ“Š Inter-annotator Agreement**
- How much do evaluators agree?

---

## Best Practices for Human Evaluation

âœ… **Clear Guidelines:** Detailed instructions for evaluators
âœ… **Multiple Annotators:** At least 3 people per item
âœ… **Quality Control:** Regular agreement checks
âœ… **Balanced Design:** Randomize order, avoid bias
âœ… **Pilot Testing:** Test your evaluation setup first

---

## ğŸš€ Group Activity: Human Evaluation with MQM

**Let's evaluate two AI-generated texts!**

ğŸ‘¥ **Your Task:**
1. Form groups of 3-4 people
2. Read both texts in your notebook
3. Rate them using MQM-lite framework:
   - Adequacy (1-5)
   - Fluency (1-5)
   - Consistency (1-5)
4. Discuss your ratings with your group
5. Record your scores in the notebook

**Time:** 10 minutes

---

# Module 5: The Complete Picture
## Combining Multiple Metrics

---

## The Correlation Question

**Key Insight:** Good automatic metrics should correlate with human judgment!

ğŸ“Š **Research Shows:**
- BLEU correlates ~0.4-0.6 with human scores
- ROUGE correlates ~0.5-0.7 for summarization
- Correlation varies by task and domain

---

## When to Use What?

**N-gram Metrics (BLEU/ROUGE):** Quick development cycles, basic quality checks
**Embedding Metrics (BERTScore):** When semantic understanding matters
**Learned Metrics (BLEURT):** High-stakes evaluation, when human correlation is critical
**Human Eval (MQM):** Final validation, nuanced quality assessment
**Combined:** Multi-metric approach for comprehensive evaluation!

---

## ğŸš€ Final Challenge: The Aha Moment

**Back to your notebooks for the grand finale!**

ğŸ‘¨â€ğŸ’» **Exercise 5:** Compare all metrics with human ratings
- Plot human MQM scores vs. all automatic metrics
- Find cases where they agree/disagree
- Discover which metric best matches human judgment

**Time:** 8 minutes

---

## Real-World Applications

**ğŸ¢ Industry Examples:**
- Google Translate uses BLEU for development
- News summarization systems rely on ROUGE
- ChatGPT was trained with RLHF (Reinforcement Learning from Human Feedback)
- Google uses MQM for translation quality assessment

**ğŸ“Š Modern Trends:**
- Embedding-based metrics becoming standard
- Learned metrics for high-stakes systems
- Human feedback integrated into training loops
- Multi-dimensional, task-specific evaluation frameworks

---

## Key Takeaways

1. **No single metric is perfect** - Use multiple approaches
2. **Context matters** - Choose metrics for your specific task
3. **Humans are irreplaceable** - For fluency and creativity
4. **Automatic metrics are practical** - For development and research
5. **Correlation is key** - Good metrics should match human judgment

---

## What's Next?

ğŸ”¬ **Emerging Metrics:**
- COMET: Neural model trained on human judgments
- MAUVE: Distribution-based metrics for open-ended generation
- UniEval: Unified framework for evaluating multiple aspects
- Task-specific evaluation frameworks
- Human-in-the-loop continuous improvement

ğŸ“š **Further Learning:**
- Experiment with different metrics
- Read recent evaluation papers
- Build your own evaluation pipeline

---

## Questions & Discussion

**What challenges have you faced in evaluating AI systems?**

**How might you apply these metrics in your own projects?**

**What other aspects of AI do you think are hard to evaluate?**

---

# Sources & References

---

## Key Papers

[BLEU: A Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040.pdf)

[ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)

[BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675)

[BLEURT: Learning Robust Metrics for Text Generation](https://arxiv.org/abs/2004.04696)

---

## Additional Resources

ğŸ“¹ **Video Explanations:**
- [BLEU Score Explained](https://www.youtube.com/watch?v=DejHQYAGb7Q)
- [Human Evaluation in NLP](https://www.youtube.com/watch?v=8rXD5-xhemo)

ğŸ“– **Tutorials:**
- [Hugging Face Evaluate Library](https://huggingface.co/docs/evaluate/)
- [NLTK BLEU Implementation](https://www.nltk.org/api/nltk.translate.bleu_score.html)

---

## Thank You!

**Ready to evaluate AI like a pro? ğŸš€**

Remember: The best evaluation combines multiple perspectivesâ€”automatic metrics for efficiency, humans for nuance.

*Keep experimenting, keep learning!*
        </textarea>
      </section>
    </div>
  </div>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.6.1/reveal.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.6.1/plugin/markdown/markdown.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.6.1/plugin/notes/notes.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.6.1/plugin/highlight/highlight.min.js"></script>

  <script>
    Reveal.initialize({
      hash: true,
      controls: true,
      progress: true,
      center: false,
      width: '100%', height: '100%', margin: 0,
      minScale: 1, maxScale: 1,
      transition: 'fade', backgroundTransition: 'fade',
      fragments: false, // simplify flow: each slide advances as a whole
      plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ],
      controlsLayout: 'bottom-right',
      controlsBackArrows: 'faded',
      slideNumber: true,
    });
  </script>
</body>
</html>

