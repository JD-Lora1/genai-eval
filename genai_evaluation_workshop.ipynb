{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "genai_evaluation_workshop.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"id": "intro"},
      "source": [
        "# Generative AI Evaluation Workshop\n",
        "\n",
        "Welcome! This notebook complements the interactive presentation on evaluation metrics for Generative AI.\n",
        "\n",
        "You'll practice with:\n",
        "- **N-gram metrics** (BLEU, ROUGE) \n",
        "- **Embedding-based metrics** (BERTScore)\n",
        "- **Learned metrics** (BLEURT)\n",
        "- **Human evaluation** using the MQM framework\n",
        "\n",
        "Each section provides guidance, code, and space for your own experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "setup"},
      "source": [
        "## Section 1: Setup\n",
        "Install and import the libraries we'll use. If you're in Google Colab, just run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "install"},
      "source": [
        "!pip -q install nltk rouge-score matplotlib seaborn bert-score pandas --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "imports"},
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "try:\n",
        "    from bert_score import score\n",
        "    bertscore_available = True\n",
        "    print('BERTScore available!')\n",
        "except ImportError:\n",
        "    print('BERTScore not available - will use simulated scores')\n",
        "    bertscore_available = False\n",
        "nltk.download('punkt')\n",
        "print('Setup complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "bleu_intro"},
      "source": [
        "## Section 2: N-gram Metrics - BLEU & ROUGE\n",
        "BLEU measures n-gram precision between a candidate text and one or more reference texts.\n",
        "ROUGE focuses more on recall and is commonly used for summarization.\n",
        "\n",
        "**Key insights:**\n",
        "- Simple, fast, and reproducible\n",
        "- But insensitive to synonyms and paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "ngram_fns"},
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def compute_bleu(candidate: str, references: list, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "    \"\"\"\n",
        "    Compute BLEU score for one candidate against one or more references.\n",
        "    - candidate: string\n",
        "    - references: list of strings\n",
        "    - weights: tuple for 1- to 4-gram weights (default uniform)\n",
        "    \"\"\"\n",
        "    cand_tokens = word_tokenize(candidate.lower())\n",
        "    ref_tokens_list = [word_tokenize(r.lower()) for r in references]\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    score = sentence_bleu(ref_tokens_list, cand_tokens, weights=weights, smoothing_function=smoothie)\n",
        "    return score\n",
        "\n",
        "def compute_rouge(ref: str, pred: str):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(ref, pred)\n",
        "    return {k: {'precision': v.precision, 'recall': v.recall, 'f1': v.fmeasure} for k, v in scores.items()}\n",
        "\n",
        "# Demo\n",
        "candidate = 'The cat is on the mat'\n",
        "references = ['There is a cat on the mat', 'The cat sits on the mat']\n",
        "print('BLEU demo:', compute_bleu(candidate, references))\n",
        "print('ROUGE demo:', compute_rouge(references[0], candidate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "ngram_ex"},
      "source": [
        "### Exercise 2.1: N-gram Metrics Exploration\n",
        "Enter your own candidate and references. Try small edits (word order, synonyms) and observe how BLEU and ROUGE behave.\n",
        "**Time:** 7 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "ngram_user"},
      "source": [
        "#@title Enter your texts\n",
        "candidate_text = 'I love machine learning'  #@param {type: 'string'}\n",
        "references_text = 'I enjoy machine learning | I love ML'  #@param {type: 'string'}\n",
        "weights_choice = 'uniform_4gram'  #@param ['uniform_4gram', 'up_to_bigrams']\n",
        "\n",
        "if weights_choice == 'uniform_4gram':\n",
        "    weights = (0.25, 0.25, 0.25, 0.25)\n",
        "else:\n",
        "    weights = (0.5, 0.5, 0.0, 0.0)\n",
        "\n",
        "references_list = [s.strip() for s in references_text.split('|') if s.strip()]\n",
        "print('Candidate:', candidate_text)\n",
        "print('References:', references_list)\n",
        "print('Weights:', weights)\n",
        "print()\n",
        "print('BLEU:', compute_bleu(candidate_text, references_list, weights=weights))\n",
        "rouge_scores = compute_rouge(references_list[0], candidate_text)\n",
        "print('ROUGE-1 F1:', rouge_scores['rouge1']['f1'])\n",
        "print('ROUGE-L F1:', rouge_scores['rougeL']['f1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "rouge_task"},
      "source": [
        "### Exercise 2.2: Compare Two Summaries\n",
        "Given a source text, evaluate two summaries using ROUGE-1 and ROUGE-L and interpret results.\n",
        "**Time:** 8 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "rouge_user"},
      "source": [
        "#@title Enter source and summaries\n",
        "source_text = 'Artificial intelligence is transforming how we work, learn, and communicate in the modern world.'  #@param {type: 'string'}\n",
        "summary_a = 'AI transforms how we work and communicate.'  #@param {type: 'string'}\n",
        "summary_b = 'Technology is changing everything.'  #@param {type: 'string'}\n",
        "\n",
        "scores_a = compute_rouge(source_text, summary_a)\n",
        "scores_b = compute_rouge(source_text, summary_b)\n",
        "print('ROUGE for Summary A:', scores_a)\n",
        "print('ROUGE for Summary B:', scores_b)\n",
        "\n",
        "# Simple visualization for F1\n",
        "labels = ['ROUGE-1', 'ROUGE-L']\n",
        "a_f1 = [scores_a['rouge1']['f1'], scores_a['rougeL']['f1']]\n",
        "b_f1 = [scores_b['rouge1']['f1'], scores_b['rougeL']['f1']]\n",
        "x = range(len(labels))\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar([i-0.15 for i in x], a_f1, width=0.3, label='Summary A')\n",
        "plt.bar([i+0.15 for i in x], b_f1, width=0.3, label='Summary B')\n",
        "plt.xticks(x, labels)\n",
        "plt.ylim(0,1)\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('ROUGE Comparison')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "bertscore_intro"},
      "source": [
        "## Section 3: BERTScore - Embedding-based Evaluation\n",
        "BERTScore uses contextual embeddings to capture semantic meaning and handle paraphrases better than n-gram metrics.\n",
        "\n",
        "**Key advantages:**\n",
        "- Understands synonyms (\"movie\" vs \"film\")\n",
        "- Captures semantic similarity\n",
        "- Higher correlation with human judgment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "bertscore_fn"},
      "source": [
        "def compute_bertscore(references, candidates, lang='en'):\n",
        "    if bertscore_available:\n",
        "        P, R, F1 = score(candidates, references, lang=lang, verbose=False)\n",
        "        return {'precision': P.mean().item(), 'recall': R.mean().item(), 'f1': F1.mean().item()}\n",
        "    else:\n",
        "        # Simulated scores for demo purposes\n",
        "        import random\n",
        "        random.seed(hash(candidates[0]) % 1000)  # Deterministic but varied\n",
        "        \n",
        "        # Simulate better semantic understanding\n",
        "        ref_words = set(references[0].lower().split())\n",
        "        cand_words = set(candidates[0].lower().split())\n",
        "        \n",
        "        # Basic overlap\n",
        "        overlap = len(ref_words & cand_words) / len(ref_words | cand_words) if ref_words | cand_words else 0\n",
        "        \n",
        "        # Semantic bonuses (simulating BERT understanding)\n",
        "        semantic_bonus = 0\n",
        "        if ('movie' in references[0].lower() and 'film' in candidates[0].lower()) or \\\n",
        "           ('film' in references[0].lower() and 'movie' in candidates[0].lower()):\n",
        "            semantic_bonus += 0.15\n",
        "        if ('excellent' in references[0].lower() and 'great' in candidates[0].lower()) or \\\n",
        "           ('great' in references[0].lower() and 'excellent' in candidates[0].lower()):\n",
        "            semantic_bonus += 0.1\n",
        "        \n",
        "        base_score = min(0.95, overlap * 0.7 + semantic_bonus + 0.2)\n",
        "        noise = (random.random() - 0.5) * 0.05\n",
        "        \n",
        "        final_score = max(0.1, base_score + noise)\n",
        "        return {\n",
        "            'precision': final_score + random.random() * 0.05,\n",
        "            'recall': final_score + random.random() * 0.05,\n",
        "            'f1': final_score\n",
        "        }\n",
        "\n",
        "# Demo\n",
        "ref = ['The movie was excellent.']\n",
        "cand1 = ['The film was excellent.']  # Should score high (synonyms)\n",
        "cand2 = ['The movie was terrible.']  # Should score lower (opposite meaning)\n",
        "print('BERTScore for synonymous text:', compute_bertscore(ref, cand1))\n",
        "print('BERTScore for different meaning:', compute_bertscore(ref, cand2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "bertscore_ex"},
      "source": [
        "### Exercise 3.1: Compare BERTScore with BLEU/ROUGE\n",
        "Test how BERTScore handles paraphrases and synonyms compared to n-gram metrics.\n",
        "**Time:** 7 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "bertscore_compare"},
      "source": [
        "# Test cases that challenge n-gram metrics\n",
        "reference = 'The car is red.'\n",
        "candidates = [\n",
        "    'The automobile is red.',      # Synonym\n",
        "    'Red is the car.',             # Word order change\n",
        "    'The car is blue.',            # Different meaning\n",
        "    'The vehicle has a red color.' # Paraphrase\n",
        "]\n",
        "\n",
        "print(f'Reference: \"{reference}\"\\n')\n",
        "print('Comparing metrics on challenging examples:\\n')\n",
        "\n",
        "results = []\n",
        "for i, cand in enumerate(candidates, 1):\n",
        "    bleu = compute_bleu(cand, [reference])\n",
        "    rouge = compute_rouge(reference, cand)\n",
        "    bert = compute_bertscore([reference], [cand])\n",
        "    \n",
        "    results.append({\n",
        "        'Candidate': cand,\n",
        "        'BLEU': bleu,\n",
        "        'ROUGE-1': rouge['rouge1']['f1'],\n",
        "        'ROUGE-L': rouge['rougeL']['f1'],\n",
        "        'BERTScore': bert['f1']\n",
        "    })\n",
        "    \n",
        "    print(f'Example {i}: \"{cand}\"')\n",
        "    print(f'  BLEU: {bleu:.3f}')\n",
        "    print(f'  ROUGE-1 F1: {rouge[\"rouge1\"][\"f1\"]:.3f}')\n",
        "    print(f'  BERTScore F1: {bert[\"f1\"]:.3f}')\n",
        "    print()\n",
        "\n",
        "# Visualize the comparison\n",
        "df = pd.DataFrame(results)\n",
        "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-L', 'BERTScore']\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "x = np.arange(len(candidates))\n",
        "width = 0.2\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax.bar(x + i*width, df[metric], width, label=metric)\n",
        "\n",
        "ax.set_xlabel('Test Cases')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Metric Comparison: Handling Synonyms and Paraphrases')\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(['Synonym', 'Word Order', 'Different Meaning', 'Paraphrase'], rotation=45)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "bleurt_intro"},
      "source": [
        "## Section 4: BLEURT - Learned Evaluation Metric\n",
        "BLEURT is trained to predict human judgments and typically correlates better with human evaluation than other automatic metrics.\n",
        "\n",
        "**Key advantages:**\n",
        "- Highest correlation with human judgments\n",
        "- Robust to noise and edge cases\n",
        "- Can be customized for specific tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "bleurt_sim"},
      "source": [
        "def compute_bleurt_simulation(reference, candidate):\n",
        "    \"\"\"\n",
        "    Simulated BLEURT scores based on heuristics.\n",
        "    In practice, you'd use the actual BLEURT model.\n",
        "    This simulation tries to mimic BLEURT's behavior.\n",
        "    \"\"\"\n",
        "    import random\n",
        "    random.seed(hash(candidate) % 1000)  # Deterministic but varied\n",
        "    \n",
        "    # Base similarity (length and word overlap)\n",
        "    ref_words = set(reference.lower().split())\n",
        "    cand_words = set(candidate.lower().split())\n",
        "    overlap = len(ref_words & cand_words) / len(ref_words | cand_words) if ref_words | cand_words else 0\n",
        "    \n",
        "    # Length penalty/bonus\n",
        "    len_ratio = min(len(candidate), len(reference)) / max(len(candidate), len(reference))\n",
        "    \n",
        "    # Simulated semantic understanding (would be actual BERT in real BLEURT)\n",
        "    semantic_bonus = 0\n",
        "    \n",
        "    # Synonym handling\n",
        "    synonyms = [\n",
        "        ('excellent', 'great'), ('movie', 'film'), ('car', 'automobile'),\n",
        "        ('delicious', 'tasty'), ('restaurant', 'eatery')\n",
        "    ]\n",
        "    \n",
        "    for syn1, syn2 in synonyms:\n",
        "        if (syn1 in reference.lower() and syn2 in candidate.lower()) or \\\n",
        "           (syn2 in reference.lower() and syn1 in candidate.lower()):\n",
        "            semantic_bonus += 0.15\n",
        "    \n",
        "    # Sentiment alignment\n",
        "    positive_words = ['excellent', 'great', 'good', 'delicious', 'wonderful']\n",
        "    negative_words = ['terrible', 'bad', 'awful', 'poor']\n",
        "    \n",
        "    ref_positive = any(word in reference.lower() for word in positive_words)\n",
        "    ref_negative = any(word in reference.lower() for word in negative_words)\n",
        "    cand_positive = any(word in candidate.lower() for word in positive_words)\n",
        "    cand_negative = any(word in candidate.lower() for word in negative_words)\n",
        "    \n",
        "    if (ref_positive and cand_positive) or (ref_negative and cand_negative):\n",
        "        semantic_bonus += 0.1\n",
        "    elif (ref_positive and cand_negative) or (ref_negative and cand_positive):\n",
        "        semantic_bonus -= 0.2\n",
        "    \n",
        "    # Final score calculation\n",
        "    base_score = (overlap * 0.5 + len_ratio * 0.2 + semantic_bonus) * 0.85\n",
        "    noise = (random.random() - 0.5) * 0.08  # Add some realistic noise\n",
        "    \n",
        "    return max(0, min(1, base_score + noise + 0.1))\n",
        "\n",
        "# Demo\n",
        "ref = 'The movie was excellent.'\n",
        "candidates = [\n",
        "    'The film was great.',          # Synonyms, positive sentiment match\n",
        "    'The movie was terrible.',      # Sentiment mismatch\n",
        "    'Excellent film.',              # Short but positive\n",
        "    'The movie was okay.'           # Neutral sentiment\n",
        "]\n",
        "\n",
        "print(f'Reference: \"{ref}\"\\n')\n",
        "for cand in candidates:\n",
        "    score = compute_bleurt_simulation(ref, cand)\n",
        "    print(f'BLEURT* score for \"{cand}\": {score:.3f}')\n",
        "print('\\n*Simulated BLEURT scores for demonstration')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "bleurt_ex"},
      "source": [
        "### Exercise 4.1: Compare All Metrics\n",
        "Now let's compare all four types of metrics on the same examples.\n",
        "**Time:** 5 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "all_metrics_compare"},
      "source": [
        "# Comprehensive comparison\n",
        "reference = 'The restaurant served delicious food with excellent service.'\n",
        "test_cases = [\n",
        "    ('Perfect match', 'The restaurant served delicious food with excellent service.'),\n",
        "    ('Synonym', 'The eatery served tasty food with great service.'),\n",
        "    ('Paraphrase', 'Excellent service and delicious food were provided by the restaurant.'),\n",
        "    ('Partial content', 'The restaurant served food.'),\n",
        "    ('Opposite meaning', 'The restaurant served terrible food with poor service.')\n",
        "]\n",
        "\n",
        "results = []\n",
        "print(f'Reference: \"{reference}\"\\n')\n",
        "\n",
        "for description, candidate in test_cases:\n",
        "    bleu = compute_bleu(candidate, [reference])\n",
        "    rouge = compute_rouge(reference, candidate)\n",
        "    bert = compute_bertscore([reference], [candidate])\n",
        "    bleurt = compute_bleurt_simulation(reference, candidate)\n",
        "    \n",
        "    results.append({\n",
        "        'Case': description,\n",
        "        'Candidate': candidate[:40] + '...' if len(candidate) > 40 else candidate,\n",
        "        'BLEU': bleu,\n",
        "        'ROUGE-1': rouge['rouge1']['f1'],\n",
        "        'ROUGE-L': rouge['rougeL']['f1'],\n",
        "        'BERTScore': bert['f1'],\n",
        "        'BLEURT*': bleurt\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print('Comprehensive Metric Comparison:')\n",
        "print(df[['Case', 'BLEU', 'ROUGE-1', 'ROUGE-L', 'BERTScore', 'BLEURT*']].round(3))\n",
        "print('\\n*Simulated BLEURT scores for demonstration')\n",
        "\n",
        "# Visualize the results\n",
        "metrics_to_plot = ['BLEU', 'ROUGE-1', 'ROUGE-L', 'BERTScore', 'BLEURT*']\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "x = np.arange(len(test_cases))\n",
        "width = 0.15\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    ax.bar(x + i*width, df[metric], width, label=metric)\n",
        "\n",
        "ax.set_xlabel('Test Cases')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('All Metrics Comparison: Different Types of Text Pairs')\n",
        "ax.set_xticks(x + width * 2)\n",
        "ax.set_xticklabels([case[0] for case in test_cases], rotation=45)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "human_loop"},
      "source": [
        "## Section 5: Human Evaluation with MQM Framework\n",
        "In your group, read two generated texts and rate them using the **MQM-lite framework**:\n",
        "- **Adequacy** (1-5): How well is the original content preserved?\n",
        "- **Fluency** (1-5): How natural and grammatically correct is the text?\n",
        "- **Consistency** (1-5): Is the text internally coherent and consistent?\n",
        "\n",
        "**Time:** 10 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "evaluation_texts"},
      "source": [
        "### Texts to Evaluate\n",
        "\n",
        "**Task:** Rate these two AI-generated restaurant reviews using MQM-lite framework.\n",
        "\n",
        "**Text A:**\n",
        "*\"The Italian restaurant downtown offers fantastic food. Their pasta dishes are expertly prepared with fresh ingredients. The service was attentive and friendly. The ambiance creates a warm, welcoming atmosphere perfect for dinner. Highly recommended for anyone seeking authentic Italian cuisine.\"*\n",
        "\n",
        "**Text B:**\n",
        "*\"Restaurant has good food. The pasta was okay and service fine. Nice place for eat dinner. They have Italian food that tastes good. Staff is helpful and place looks nice. Would go again maybe.\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "ratings_space"},
      "source": [
        "### Record your MQM ratings here\n",
        "\n",
        "**Group Discussion Questions:**\n",
        "1. Which text better preserves the information about the restaurant?\n",
        "2. Which text sounds more natural and grammatically correct?\n",
        "3. Which text is more internally consistent?\n",
        "4. Do you and your group members agree on the ratings? Where do you disagree?\n",
        "\n",
        "**Text A Ratings:**\n",
        "- Group average Adequacy: ____\n",
        "- Group average Fluency: ____\n",
        "- Group average Consistency: ____\n",
        "\n",
        "**Text B Ratings:**\n",
        "- Group average Adequacy: ____\n",
        "- Group average Fluency: ____\n",
        "- Group average Consistency: ____\n",
        "\n",
        "**Notes:** (Add any interesting observations or disagreements)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "human_compare"},
      "source": [
        "# Enter your numeric human ratings and compare with automatic metrics\n",
        "\n",
        "# Text A ratings (1-5 scale)\n",
        "text_a_adequacy = 4.5    #@param {type:'number'}\n",
        "text_a_fluency = 4.2     #@param {type:'number'}\n",
        "text_a_consistency = 4.0 #@param {type:'number'}\n",
        "\n",
        "# Text B ratings (1-5 scale)\n",
        "text_b_adequacy = 3.0    #@param {type:'number'}\n",
        "text_b_fluency = 2.5     #@param {type:'number'}\n",
        "text_b_consistency = 2.8 #@param {type:'number'}\n",
        "\n",
        "# Define the texts\n",
        "text_a = \"The Italian restaurant downtown offers fantastic food. Their pasta dishes are expertly prepared with fresh ingredients. The service was attentive and friendly. The ambiance creates a warm, welcoming atmosphere perfect for dinner. Highly recommended for anyone seeking authentic Italian cuisine.\"\n",
        "text_b = \"Restaurant has good food. The pasta was okay and service fine. Nice place for eat dinner. They have Italian food that tastes good. Staff is helpful and place looks nice. Would go again maybe.\"\n",
        "\n",
        "# Reference text (what we're comparing against)\n",
        "reference = \"The Italian restaurant serves excellent food with great service and nice atmosphere.\"\n",
        "\n",
        "# Calculate automatic metrics for both texts\n",
        "# Text A metrics\n",
        "bleu_a = compute_bleu(text_a, [reference])\n",
        "rouge_a = compute_rouge(reference, text_a)\n",
        "bert_a = compute_bertscore([reference], [text_a])\n",
        "bleurt_a = compute_bleurt_simulation(reference, text_a)\n",
        "\n",
        "# Text B metrics\n",
        "bleu_b = compute_bleu(text_b, [reference])\n",
        "rouge_b = compute_rouge(reference, text_b)\n",
        "bert_b = compute_bertscore([reference], [text_b])\n",
        "bleurt_b = compute_bleurt_simulation(reference, text_b)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = {\n",
        "    'Text': ['A', 'B'],\n",
        "    'Human_Adequacy': [text_a_adequacy, text_b_adequacy],\n",
        "    'Human_Fluency': [text_a_fluency, text_b_fluency],\n",
        "    'Human_Consistency': [text_a_consistency, text_b_consistency],\n",
        "    'Human_Average': [(text_a_adequacy + text_a_fluency + text_a_consistency)/3,\n",
        "                      (text_b_adequacy + text_b_fluency + text_b_consistency)/3],\n",
        "    'BLEU': [bleu_a, bleu_b],\n",
        "    'ROUGE-1': [rouge_a['rouge1']['f1'], rouge_b['rouge1']['f1']],\n",
        "    'ROUGE-L': [rouge_a['rougeL']['f1'], rouge_b['rougeL']['f1']],\n",
        "    'BERTScore': [bert_a['f1'], bert_b['f1']],\n",
        "    'BLEURT*': [bleurt_a, bleurt_b]\n",
        "}\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "print('Human vs Automatic Metrics Comparison:')\n",
        "print(df_comparison.round(3))\n",
        "print('\\n*Simulated BLEURT scores')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "final_viz"},
      "source": [
        "# The Final Visualization: Human vs Automatic Metrics\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Human evaluation breakdown\n",
        "human_metrics = ['Adequacy', 'Fluency', 'Consistency']\n",
        "text_a_human = [text_a_adequacy, text_a_fluency, text_a_consistency]\n",
        "text_b_human = [text_b_adequacy, text_b_fluency, text_b_consistency]\n",
        "\n",
        "x = np.arange(len(human_metrics))\n",
        "width = 0.35\n",
        "\n",
        "ax1.bar(x - width/2, text_a_human, width, label='Text A', color='skyblue')\n",
        "ax1.bar(x + width/2, text_b_human, width, label='Text B', color='lightcoral')\n",
        "ax1.set_xlabel('MQM Dimensions')\n",
        "ax1.set_ylabel('Human Rating (1-5)')\n",
        "ax1.set_title('Human Evaluation (MQM Framework)')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(human_metrics)\n",
        "ax1.legend()\n",
        "ax1.set_ylim(0, 5)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Automatic metrics comparison\n",
        "auto_metrics = ['BLEU', 'ROUGE-1', 'ROUGE-L', 'BERTScore', 'BLEURT*']\n",
        "text_a_auto = [bleu_a, rouge_a['rouge1']['f1'], rouge_a['rougeL']['f1'], bert_a['f1'], bleurt_a]\n",
        "text_b_auto = [bleu_b, rouge_b['rouge1']['f1'], rouge_b['rougeL']['f1'], bert_b['f1'], bleurt_b]\n",
        "\n",
        "x2 = np.arange(len(auto_metrics))\n",
        "ax2.bar(x2 - width/2, text_a_auto, width, label='Text A', color='skyblue')\n",
        "ax2.bar(x2 + width/2, text_b_auto, width, label='Text B', color='lightcoral')\n",
        "ax2.set_xlabel('Automatic Metrics')\n",
        "ax2.set_ylabel('Score (0-1)')\n",
        "ax2.set_title('Automatic Metrics Comparison')\n",
        "ax2.set_xticks(x2)\n",
        "ax2.set_xticklabels(auto_metrics, rotation=45)\n",
        "ax2.legend()\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis\n",
        "print(\"\\nðŸŽ¯ THE AHA MOMENT:\")\n",
        "print(\"Compare the patterns between human ratings and automatic metrics:\")\n",
        "print(f\"- Which automatic metric best matches your human judgment?\")\n",
        "print(f\"- Where do the metrics disagree with human ratings?\")\n",
        "print(f\"- What aspects do humans capture that automatic metrics miss?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "final_challenge"},
      "source": [
        "## Concluding Challenge: Your Own Evaluation Pipeline\n",
        "\n",
        "**Task:** Choose a text generation scenario (translation, summarization, creative writing) and:\n",
        "\n",
        "1. **Create 2-3 candidate outputs**\n",
        "2. **Define appropriate references** (or gold standard)\n",
        "3. **Apply all metrics:** BLEU, ROUGE, BERTScore, BLEURT, and human evaluation\n",
        "4. **Analyze the results:** Which metrics agree? Which disagree? Why?\n",
        "5. **Choose your evaluation strategy:** Which combination of metrics would you use for this task?\n",
        "\n",
        "**Key Questions to Consider:**\n",
        "- What aspects of quality matter most for your chosen task?\n",
        "- Which metrics capture those aspects best?\n",
        "- How would you combine automatic and human evaluation in practice?\n",
        "- What are the trade-offs between speed, cost, and evaluation quality?\n",
        "\n",
        "**Bonus Challenge:** Try implementing a simple ensemble metric that combines multiple automatic metrics. How does it correlate with human judgment?\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ“ Congratulations!\n",
        "\n",
        "You've now experienced the full spectrum of generative AI evaluation:\n",
        "- **N-gram metrics** for quick, reproducible evaluation\n",
        "- **Embedding metrics** for semantic understanding\n",
        "- **Learned metrics** for human-like judgment\n",
        "- **Human evaluation** for nuanced quality assessment\n",
        "\n",
        "**Remember:** The best evaluation strategy combines multiple perspectives. Choose your metrics based on your task, resources, and quality requirements!\n",
        "\n",
        "*Keep experimenting, keep learning!* ðŸš€"
      ]
    }
  ]
}
